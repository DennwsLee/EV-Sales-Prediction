{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ4Xt1uf1LT7"
      },
      "source": [
        "# Final Project Phase 2 Summary\n",
        "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 2 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
        "\n",
        "Note: To edit a Markdown cell, double-click on its text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjB_SbWY1LUB"
      },
      "source": [
        "## Jupyter Notebook Quick Tips\n",
        "Here are some quick formatting tips to get you started with Jupyter Notebooks. This is by no means exhaustive, and there are plenty of articles to highlight other things that can be done. We recommend using HTML syntax for Markdown but there is also Markdown syntax that is more streamlined and might be preferable.\n",
        "<a href = \"https://towardsdatascience.com/markdown-cells-jupyter-notebook-d3bea8416671\">Here's an article</a> that goes into more detail. (Double-click on cell to see syntax)\n",
        "\n",
        "# Heading 1\n",
        "## Heading 2\n",
        "### Heading 3\n",
        "#### Heading 4\n",
        "<br>\n",
        "<b>BoldText</b> or <i>ItalicText</i>\n",
        "<br> <br>\n",
        "Math Formulas: $x^2 + y^2 = 1$\n",
        "<br> <br>\n",
        "Line Breaks are done using br enclosed in < >.\n",
        "<br><br>\n",
        "Hyperlinks are done with: <a> https://www.google.com </a> or\n",
        "<a href=\"http://www.google.com\">Google</a><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb9oVjpRDswQ"
      },
      "source": [
        "# Data Collection and Cleaning\n",
        "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file.\n",
        "\n",
        "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH_3kaMKb1l_"
      },
      "source": [
        "# Partner Names\n",
        "Please add both you and your partner's names below. If you are working alone, obviously it would only be your name.\n",
        "*   Partner 1: Allen You\n",
        "*   Partner 2 (If Applicable): Dennis Lee\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dp7Pm-Suh3d"
      },
      "source": [
        "## Data Sources\n",
        "Include sources (as links) to your datasets. Add any additional data sources if needed. Clearly indicate if a data source is different from one submitted in your Phase I, as we will check that it satisfies the requirements.\n",
        "*   Downloaded Dataset Source: https://www.fueleconomy.gov/feg/download.shtml\n",
        "*   Web Collection #1 Source:\n",
        "*   Web Collection #2 Source:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mRjxZDbE1tj"
      },
      "source": [
        "## Downloaded Dataset Requirement\n",
        "\n",
        "Fill in the predefined functions with your data scraping/parsing code. You may modify/rename each function as you seem fit, but you must provide at least 3 separate functions that clean each of your required datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "794L4vGXFdYw"
      },
      "source": [
        "## Web Collection Requirement \\#1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vXwpJObDFiWM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>National</th>\n",
              "      <th>California</th>\n",
              "      <th>Colorado</th>\n",
              "      <th>Florida</th>\n",
              "      <th>Massachusetts</th>\n",
              "      <th>Minnesota</th>\n",
              "      <th>New York</th>\n",
              "      <th>Ohio</th>\n",
              "      <th>Texas</th>\n",
              "      <th>Washington</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>1.524</td>\n",
              "      <td>1.772</td>\n",
              "      <td>1.602</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.550</td>\n",
              "      <td>1.680</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.479</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>1.466</td>\n",
              "      <td>1.684</td>\n",
              "      <td>1.507</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.493</td>\n",
              "      <td>1.568</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002</th>\n",
              "      <td>1.382</td>\n",
              "      <td>1.555</td>\n",
              "      <td>1.390</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.369</td>\n",
              "      <td>1.489</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.312</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>1.601</td>\n",
              "      <td>1.874</td>\n",
              "      <td>1.568</td>\n",
              "      <td>1.579</td>\n",
              "      <td>1.635</td>\n",
              "      <td>1.538</td>\n",
              "      <td>1.730</td>\n",
              "      <td>1.546</td>\n",
              "      <td>1.488</td>\n",
              "      <td>1.687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>1.891</td>\n",
              "      <td>2.161</td>\n",
              "      <td>1.853</td>\n",
              "      <td>1.911</td>\n",
              "      <td>1.904</td>\n",
              "      <td>1.797</td>\n",
              "      <td>2.037</td>\n",
              "      <td>1.834</td>\n",
              "      <td>1.768</td>\n",
              "      <td>1.987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2005</th>\n",
              "      <td>2.312</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.298</td>\n",
              "      <td>2.354</td>\n",
              "      <td>2.306</td>\n",
              "      <td>2.171</td>\n",
              "      <td>2.453</td>\n",
              "      <td>2.251</td>\n",
              "      <td>2.215</td>\n",
              "      <td>2.407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006</th>\n",
              "      <td>2.615</td>\n",
              "      <td>2.851</td>\n",
              "      <td>2.586</td>\n",
              "      <td>2.634</td>\n",
              "      <td>2.617</td>\n",
              "      <td>2.517</td>\n",
              "      <td>2.796</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.508</td>\n",
              "      <td>2.749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>2.846</td>\n",
              "      <td>3.124</td>\n",
              "      <td>2.836</td>\n",
              "      <td>2.841</td>\n",
              "      <td>2.774</td>\n",
              "      <td>2.780</td>\n",
              "      <td>2.998</td>\n",
              "      <td>2.819</td>\n",
              "      <td>2.707</td>\n",
              "      <td>3.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008</th>\n",
              "      <td>3.305</td>\n",
              "      <td>3.569</td>\n",
              "      <td>3.218</td>\n",
              "      <td>3.335</td>\n",
              "      <td>3.238</td>\n",
              "      <td>3.132</td>\n",
              "      <td>3.509</td>\n",
              "      <td>3.216</td>\n",
              "      <td>3.174</td>\n",
              "      <td>3.459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2009</th>\n",
              "      <td>2.397</td>\n",
              "      <td>2.718</td>\n",
              "      <td>2.286</td>\n",
              "      <td>2.408</td>\n",
              "      <td>2.353</td>\n",
              "      <td>2.315</td>\n",
              "      <td>2.564</td>\n",
              "      <td>2.346</td>\n",
              "      <td>2.258</td>\n",
              "      <td>2.618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010</th>\n",
              "      <td>2.834</td>\n",
              "      <td>3.137</td>\n",
              "      <td>2.712</td>\n",
              "      <td>2.823</td>\n",
              "      <td>2.800</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.998</td>\n",
              "      <td>2.766</td>\n",
              "      <td>2.689</td>\n",
              "      <td>3.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>3.576</td>\n",
              "      <td>3.866</td>\n",
              "      <td>3.447</td>\n",
              "      <td>3.550</td>\n",
              "      <td>3.590</td>\n",
              "      <td>3.549</td>\n",
              "      <td>3.803</td>\n",
              "      <td>3.504</td>\n",
              "      <td>3.429</td>\n",
              "      <td>3.768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012</th>\n",
              "      <td>3.686</td>\n",
              "      <td>4.090</td>\n",
              "      <td>3.535</td>\n",
              "      <td>3.634</td>\n",
              "      <td>3.721</td>\n",
              "      <td>3.569</td>\n",
              "      <td>3.941</td>\n",
              "      <td>3.621</td>\n",
              "      <td>3.492</td>\n",
              "      <td>3.894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013</th>\n",
              "      <td>3.576</td>\n",
              "      <td>3.934</td>\n",
              "      <td>3.470</td>\n",
              "      <td>3.572</td>\n",
              "      <td>3.616</td>\n",
              "      <td>3.501</td>\n",
              "      <td>3.837</td>\n",
              "      <td>3.510</td>\n",
              "      <td>3.389</td>\n",
              "      <td>3.691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014</th>\n",
              "      <td>3.442</td>\n",
              "      <td>3.800</td>\n",
              "      <td>3.394</td>\n",
              "      <td>3.427</td>\n",
              "      <td>3.485</td>\n",
              "      <td>3.301</td>\n",
              "      <td>3.703</td>\n",
              "      <td>3.384</td>\n",
              "      <td>3.231</td>\n",
              "      <td>3.608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015</th>\n",
              "      <td>2.513</td>\n",
              "      <td>3.210</td>\n",
              "      <td>2.402</td>\n",
              "      <td>2.433</td>\n",
              "      <td>2.457</td>\n",
              "      <td>2.390</td>\n",
              "      <td>2.663</td>\n",
              "      <td>2.401</td>\n",
              "      <td>2.253</td>\n",
              "      <td>2.744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>2.252</td>\n",
              "      <td>2.786</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.219</td>\n",
              "      <td>2.223</td>\n",
              "      <td>2.092</td>\n",
              "      <td>2.363</td>\n",
              "      <td>2.188</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>2.530</td>\n",
              "      <td>3.085</td>\n",
              "      <td>2.431</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.513</td>\n",
              "      <td>2.389</td>\n",
              "      <td>2.620</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.296</td>\n",
              "      <td>2.914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>2.817</td>\n",
              "      <td>3.554</td>\n",
              "      <td>2.752</td>\n",
              "      <td>2.708</td>\n",
              "      <td>2.827</td>\n",
              "      <td>2.655</td>\n",
              "      <td>2.898</td>\n",
              "      <td>2.620</td>\n",
              "      <td>2.532</td>\n",
              "      <td>3.272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019</th>\n",
              "      <td>2.686</td>\n",
              "      <td>3.674</td>\n",
              "      <td>2.653</td>\n",
              "      <td>2.507</td>\n",
              "      <td>2.662</td>\n",
              "      <td>2.498</td>\n",
              "      <td>2.726</td>\n",
              "      <td>2.535</td>\n",
              "      <td>2.350</td>\n",
              "      <td>3.184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>2.260</td>\n",
              "      <td>3.133</td>\n",
              "      <td>2.334</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.236</td>\n",
              "      <td>2.051</td>\n",
              "      <td>2.326</td>\n",
              "      <td>2.083</td>\n",
              "      <td>1.901</td>\n",
              "      <td>2.732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021</th>\n",
              "      <td>3.094</td>\n",
              "      <td>4.093</td>\n",
              "      <td>3.258</td>\n",
              "      <td>2.950</td>\n",
              "      <td>3.007</td>\n",
              "      <td>2.865</td>\n",
              "      <td>3.099</td>\n",
              "      <td>2.927</td>\n",
              "      <td>2.722</td>\n",
              "      <td>3.513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022</th>\n",
              "      <td>4.064</td>\n",
              "      <td>5.405</td>\n",
              "      <td>3.896</td>\n",
              "      <td>3.819</td>\n",
              "      <td>4.105</td>\n",
              "      <td>3.825</td>\n",
              "      <td>4.110</td>\n",
              "      <td>3.838</td>\n",
              "      <td>3.558</td>\n",
              "      <td>4.657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023</th>\n",
              "      <td>3.668</td>\n",
              "      <td>4.910</td>\n",
              "      <td>3.744</td>\n",
              "      <td>3.478</td>\n",
              "      <td>3.610</td>\n",
              "      <td>3.493</td>\n",
              "      <td>3.681</td>\n",
              "      <td>3.454</td>\n",
              "      <td>3.187</td>\n",
              "      <td>4.571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        National  California  Colorado  Florida  Massachusetts  Minnesota  \\\n",
              "Year                                                                        \n",
              "  2000     1.524       1.772     1.602      NaN            NaN      1.550   \n",
              "  2001     1.466       1.684     1.507      NaN            NaN      1.493   \n",
              "  2002     1.382       1.555     1.390      NaN            NaN      1.369   \n",
              "  2003     1.601       1.874     1.568    1.579          1.635      1.538   \n",
              "  2004     1.891       2.161     1.853    1.911          1.904      1.797   \n",
              "  2005     2.312       2.515     2.298    2.354          2.306      2.171   \n",
              "  2006     2.615       2.851     2.586    2.634          2.617      2.517   \n",
              "  2007     2.846       3.124     2.836    2.841          2.774      2.780   \n",
              "  2008     3.305       3.569     3.218    3.335          3.238      3.132   \n",
              "  2009     2.397       2.718     2.286    2.408          2.353      2.315   \n",
              "  2010     2.834       3.137     2.712    2.823          2.800      2.786   \n",
              "  2011     3.576       3.866     3.447    3.550          3.590      3.549   \n",
              "  2012     3.686       4.090     3.535    3.634          3.721      3.569   \n",
              "  2013     3.576       3.934     3.470    3.572          3.616      3.501   \n",
              "  2014     3.442       3.800     3.394    3.427          3.485      3.301   \n",
              "  2015     2.513       3.210     2.402    2.433          2.457      2.390   \n",
              "  2016     2.252       2.786     2.142    2.219          2.223      2.092   \n",
              "  2017     2.530       3.085     2.431    2.487          2.513      2.389   \n",
              "  2018     2.817       3.554     2.752    2.708          2.827      2.655   \n",
              "  2019     2.686       3.674     2.653    2.507          2.662      2.498   \n",
              "  2020     2.260       3.133     2.334    2.152          2.236      2.051   \n",
              "  2021     3.094       4.093     3.258    2.950          3.007      2.865   \n",
              "  2022     4.064       5.405     3.896    3.819          4.105      3.825   \n",
              "  2023     3.668       4.910     3.744    3.478          3.610      3.493   \n",
              "\n",
              "        New York   Ohio  Texas  Washington  \n",
              "Year                                        \n",
              "  2000     1.680    NaN  1.479         NaN  \n",
              "  2001     1.568    NaN  1.372         NaN  \n",
              "  2002     1.489    NaN  1.312         NaN  \n",
              "  2003     1.730  1.546  1.488       1.687  \n",
              "  2004     2.037  1.834  1.768       1.987  \n",
              "  2005     2.453  2.251  2.215       2.407  \n",
              "  2006     2.796  2.532  2.508       2.749  \n",
              "  2007     2.998  2.819  2.707       3.009  \n",
              "  2008     3.509  3.216  3.174       3.459  \n",
              "  2009     2.564  2.346  2.258       2.618  \n",
              "  2010     2.998  2.766  2.689       3.055  \n",
              "  2011     3.803  3.504  3.429       3.768  \n",
              "  2012     3.941  3.621  3.492       3.894  \n",
              "  2013     3.837  3.510  3.389       3.691  \n",
              "  2014     3.703  3.384  3.231       3.608  \n",
              "  2015     2.663  2.401  2.253       2.744  \n",
              "  2016     2.363  2.188  2.018       2.533  \n",
              "  2017     2.620  2.386  2.296       2.914  \n",
              "  2018     2.898  2.620  2.532       3.272  \n",
              "  2019     2.726  2.535  2.350       3.184  \n",
              "  2020     2.326  2.083  1.901       2.732  \n",
              "  2021     3.099  2.927  2.722       3.513  \n",
              "  2022     4.110  3.838  3.558       4.657  \n",
              "  2023     3.681  3.454  3.187       4.571  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "def web_parser1():\n",
        "  # Requesting the html string using requests module and creating soup object from it\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0_pte_nus_dpg&f=m\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "\n",
        "  # Locating the table tag, which stores all the rows\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "\n",
        "  # Locating all the rows\n",
        "  rows = table.find_all(\"tr\")\n",
        "\n",
        "  # Inner and outer list used to create pandas dataframe\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "\n",
        "  # Iterating through the rows, adding to the inner list the text\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "\n",
        "  # Creating list for header/columns\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "\n",
        "  # Creating dataframe from list of lists\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "\n",
        "  # Masking the dataframe to eliminate rows of empty separation from html source code\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "\n",
        "  # Setting the index to the year\n",
        "  df.set_index('Year', inplace=True)\n",
        "\n",
        "  # Setting the dataframe to only contain years past year 2000\n",
        "  df = df.iloc[7:, :]\n",
        "\n",
        "  # Changing only the gas prices from string to float to use aggregate mean\n",
        "  df = df[df!= ''].astype(float)\n",
        "\n",
        "  # Creating new column to hold the mean of ALL months, or the yearly average\n",
        "  df[\"National\"] = df.mean(axis = 1).round(3)\n",
        "\n",
        "  # New dataframe that holds only these yearly averages to be used in the final dataframe\n",
        "  # Only U.S values here; the entire code is repeated with minor changes and different request urls to capture the\n",
        "  # data for states as well in order to compile a yearly average for the U.S and individual states\n",
        "  USdf = df[\"National\"]\n",
        "\n",
        "  # Same process for state of California; adjusted iloc because year default starts at 2000\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SCA_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"California\"] = df.mean(axis = 1).round(3)\n",
        "  CAdf = df[\"California\"]\n",
        "\n",
        "  # Same process for state of Colorado\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SCO_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Colorado\"] = df.mean(axis = 1).round(3)\n",
        "  COdf = df[\"Colorado\"]\n",
        "\n",
        "  # Same process for state of Florida\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SFL_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Florida\"] = df.mean(axis = 1).round(3)\n",
        "  FLdf = df[\"Florida\"]\n",
        "\n",
        "  # Same process for state of Massachusetts\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SMA_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Massachusetts\"] = df.mean(axis = 1).round(3)\n",
        "  MAdf = df[\"Massachusetts\"]\n",
        "\n",
        "  # Same process for state of Minnesota\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SMN_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Minnesota\"] = df.mean(axis = 1).round(3)\n",
        "  MNdf = df[\"Minnesota\"]\n",
        "\n",
        "  # Same process for state of New York\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SNY_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"New York\"] = df.mean(axis = 1).round(3)\n",
        "  NYdf = df[\"New York\"]\n",
        "\n",
        "  # Same process for state of Ohio\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SOH_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Ohio\"] = df.mean(axis = 1).round(3)\n",
        "  OHdf = df[\"Ohio\"]\n",
        "\n",
        "  # Same process for state of Texas\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_STX_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Texas\"] = df.mean(axis = 1).round(3)\n",
        "  TXdf = df[\"Texas\"]\n",
        "\n",
        "  # Same process for state of Washington\n",
        "  response = requests.get(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPM0_PTE_SWA_DPG&f=M\")\n",
        "  soup = BeautifulSoup(response.text, features = \"lxml\")\n",
        "  table = soup.find(\"table\", {\"class\" : \"FloatTitle\"})\n",
        "  rows = table.find_all(\"tr\")\n",
        "  listofrows = []\n",
        "  innerrow = []\n",
        "  for row in rows[1:]:\n",
        "    innerrow = []\n",
        "    for data in row.find_all(\"td\"):\n",
        "      innerrow.append(data.text)\n",
        "    listofrows.append(innerrow)\n",
        "  header = [tag.text for tag in rows[0] if tag.text != \" \"]\n",
        "  df = pd.DataFrame(listofrows, columns = header)\n",
        "  df = df[df.iloc[:,0] != \"\"]\n",
        "  df.set_index('Year', inplace=True)\n",
        "  df = df.iloc[0:, :]\n",
        "  df = df[(df!= '') & (df!= \"NA\")].astype(float)\n",
        "  df[\"Washington\"] = df.mean(axis = 1).round(3)\n",
        "  WAdf = df[\"Washington\"]\n",
        "\n",
        "  # Creating the final dataframe by concatenating the individual aggregated average columns for each state yearly\n",
        "  updateddf = pd.concat([USdf, CAdf, COdf, FLdf, MAdf, MNdf, NYdf, OHdf, TXdf, WAdf], axis = 1)\n",
        "\n",
        "  # Writing the data to a csv file\n",
        "  updateddf.to_csv(\"updatedfuelprices.csv\", index = True)\n",
        "  return updateddf\n",
        "\n",
        "############ Function Call ############\n",
        "(web_parser1())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDD6sMsCXRxc"
      },
      "source": [
        "## Web Collection Requirement \\#2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HAkUOqMgXQJG"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Station Name</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Network</th>\n",
              "      <th>Pricing</th>\n",
              "      <th>Open Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Home Depot</td>\n",
              "      <td>San Luis Obispo</td>\n",
              "      <td>CA</td>\n",
              "      <td>Non-Networked</td>\n",
              "      <td>Free</td>\n",
              "      <td>2000-04-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>San Luis Obispo Promenade - Bed Bath &amp; Beyond</td>\n",
              "      <td>San Luis Obispo</td>\n",
              "      <td>CA</td>\n",
              "      <td>Non-Networked</td>\n",
              "      <td>Free</td>\n",
              "      <td>2000-04-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>Marsh Street Parking Structure</td>\n",
              "      <td>San Luis Obispo</td>\n",
              "      <td>CA</td>\n",
              "      <td>Non-Networked</td>\n",
              "      <td>Free</td>\n",
              "      <td>2000-04-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>California State University - Northridge</td>\n",
              "      <td>Northridge</td>\n",
              "      <td>CA</td>\n",
              "      <td>Non-Networked</td>\n",
              "      <td>Free</td>\n",
              "      <td>2002-01-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>California State University - Northridge</td>\n",
              "      <td>Northridge</td>\n",
              "      <td>CA</td>\n",
              "      <td>Non-Networked</td>\n",
              "      <td>Free; parking is $8 per day.</td>\n",
              "      <td>2002-01-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36667</th>\n",
              "      <td>20673 Tracy Avenue (US-FQC-K3P-1A)</td>\n",
              "      <td>Buttonwillow</td>\n",
              "      <td>CA</td>\n",
              "      <td>RIVIAN_ADVENTURE</td>\n",
              "      <td>Free</td>\n",
              "      <td>2023-11-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36672</th>\n",
              "      <td>20673 Tracy Avenue (US-FQC-K3P-3A)</td>\n",
              "      <td>Buttonwillow</td>\n",
              "      <td>CA</td>\n",
              "      <td>RIVIAN_ADVENTURE</td>\n",
              "      <td>Free</td>\n",
              "      <td>2023-11-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36669</th>\n",
              "      <td>20673 Tracy Avenue (US-FQC-K3P-2A)</td>\n",
              "      <td>Buttonwillow</td>\n",
              "      <td>CA</td>\n",
              "      <td>RIVIAN_ADVENTURE</td>\n",
              "      <td>Free</td>\n",
              "      <td>2023-11-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36676</th>\n",
              "      <td>BVC 61NB - CT4020</td>\n",
              "      <td>Boston</td>\n",
              "      <td>MA</td>\n",
              "      <td>ChargePoint Network</td>\n",
              "      <td>Free</td>\n",
              "      <td>2023-11-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36677</th>\n",
              "      <td>BVC 75NB - RIGHT</td>\n",
              "      <td>Boston</td>\n",
              "      <td>MA</td>\n",
              "      <td>ChargePoint Network</td>\n",
              "      <td>Free</td>\n",
              "      <td>2023-11-13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36643 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Station Name             City State  \\\n",
              "88                                        Home Depot  San Luis Obispo    CA   \n",
              "89     San Luis Obispo Promenade - Bed Bath & Beyond  San Luis Obispo    CA   \n",
              "296                   Marsh Street Parking Structure  San Luis Obispo    CA   \n",
              "29          California State University - Northridge       Northridge    CA   \n",
              "30          California State University - Northridge       Northridge    CA   \n",
              "...                                              ...              ...   ...   \n",
              "36667             20673 Tracy Avenue (US-FQC-K3P-1A)     Buttonwillow    CA   \n",
              "36672             20673 Tracy Avenue (US-FQC-K3P-3A)     Buttonwillow    CA   \n",
              "36669             20673 Tracy Avenue (US-FQC-K3P-2A)     Buttonwillow    CA   \n",
              "36676                              BVC 61NB - CT4020           Boston    MA   \n",
              "36677                               BVC 75NB - RIGHT           Boston    MA   \n",
              "\n",
              "                   Network                       Pricing  Open Date  \n",
              "88           Non-Networked                          Free 2000-04-15  \n",
              "89           Non-Networked                          Free 2000-04-15  \n",
              "296          Non-Networked                          Free 2000-04-15  \n",
              "29           Non-Networked                          Free 2002-01-15  \n",
              "30           Non-Networked  Free; parking is $8 per day. 2002-01-15  \n",
              "...                    ...                           ...        ...  \n",
              "36667     RIVIAN_ADVENTURE                          Free 2023-11-12  \n",
              "36672     RIVIAN_ADVENTURE                          Free 2023-11-12  \n",
              "36669     RIVIAN_ADVENTURE                          Free 2023-11-12  \n",
              "36676  ChargePoint Network                          Free 2023-11-13  \n",
              "36677  ChargePoint Network                          Free 2023-11-13  \n",
              "\n",
              "[36643 rows x 6 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "def web_parser2():\n",
        "    # Requesting the api with the following parameters to filter/clean the data intially without having to later:\n",
        "    # Fuel type is set to ElEC to filter by electric charging stations ONLY\n",
        "    # The states parameter are set to the states that were found in dataset #2 for ease of comparison and consistency\n",
        "    # The limit is set to all as we want to see all of the stations that match our query\n",
        "    # Access parameter is set to public because we want the stations that are accessible by the public\n",
        "    # Lastly, the status parameter is set to E,T, meaning active/temporarily disabled stations, respectively\n",
        "    response = requests.get(\"https://developer.nrel.gov/api/alt-fuel-stations/v1.json?api_key=DEMO_KEY&fuel_type=ELEC&state=CA,CO,FL,MA,MN,NY,OH,TX,WA&limit=all&access=public&status=E,T\")\n",
        "\n",
        "    # Creating the list skeletons that will be used as input for columns in dataframe\n",
        "    stationnames = []\n",
        "    cities = []\n",
        "    states = []\n",
        "    networks = []\n",
        "    pricings = []\n",
        "    opendates = []\n",
        "\n",
        "    # Iterating through the stations from the response in json, then appending each of the responses we want\n",
        "    # to their respective list skeleton\n",
        "    for station in response.json()[\"fuel_stations\"]:\n",
        "      stationnames.append(station[\"station_name\"])\n",
        "      cities.append(station[\"city\"])\n",
        "      states.append(station[\"state\"])\n",
        "      pricings.append(station[\"ev_pricing\"])\n",
        "      opendates.append(station[\"open_date\"])\n",
        "      networks.append(station[\"ev_network\"])\n",
        "\n",
        "    # Creating an empty dataframe\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    # Adding columns to the dataframe through the filled list skeletons\n",
        "    df[\"Station Name\"] = stationnames\n",
        "    df[\"City\"] = cities\n",
        "    df[\"State\"] = states\n",
        "    df[\"Network\"] = networks\n",
        "    df[\"Pricing\"] = pricings\n",
        "    df[\"Open Date\"] = opendates\n",
        "\n",
        "    # Eliminating the stations that have an opening date before year 2000\n",
        "    df = df[df['Open Date'].astype(str).str.startswith('2')]\n",
        "\n",
        "    # Setting the opening date column to type datetime in order to sort\n",
        "    df[\"Open Date\"] = pd.to_datetime(df[\"Open Date\"])\n",
        "\n",
        "    # Sorting the rows by the open date of each station\n",
        "    df.sort_values('Open Date', inplace=True)\n",
        "\n",
        "    # Renaming the entries that have no cost as \"Free\" using np.where\n",
        "    df[\"Pricing\"] = np.where(pd.isna(df[\"Pricing\"]), \"Free\", df[\"Pricing\"])\n",
        "\n",
        "    # Writing the dataframe to a csv file\n",
        "    df.to_csv(\"updatedelectricstations\", index = True)\n",
        "\n",
        "    return df\n",
        "\n",
        "############ Function Call ############\n",
        "web_parser2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezovwa1tp0we"
      },
      "source": [
        "## Additional Dataset Parsing/Cleaning Functions\n",
        "\n",
        "Write any supplemental (optional) functions here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "f4-s72RNuKLR"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Closing Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Nov 10, 2023</th>\n",
              "      <td>214.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct 31, 2023</th>\n",
              "      <td>214.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sep 30, 2023</th>\n",
              "      <td>200.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Aug 31, 2023</th>\n",
              "      <td>250.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jul 31, 2023</th>\n",
              "      <td>258.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mar 01, 2016</th>\n",
              "      <td>15.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Feb 01, 2016</th>\n",
              "      <td>12.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jan 01, 2016</th>\n",
              "      <td>12.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dec 01, 2015</th>\n",
              "      <td>16.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oct 31, 2015</th>\n",
              "      <td>15.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>98 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Closing Price\n",
              "Date                      \n",
              "Nov 10, 2023        214.65\n",
              "Oct 31, 2023        214.65\n",
              "Sep 30, 2023        200.84\n",
              "Aug 31, 2023        250.22\n",
              "Jul 31, 2023        258.08\n",
              "...                    ...\n",
              "Mar 01, 2016         15.32\n",
              "Feb 01, 2016         12.80\n",
              "Jan 01, 2016         12.75\n",
              "Dec 01, 2015         16.00\n",
              "Oct 31, 2015         15.35\n",
              "\n",
              "[98 rows x 1 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def extra_source1():\n",
        "    # Load the HTML content from the given file path\n",
        "    with open(\"TSLA1.html\", 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "\n",
        "    # Find the table headers\n",
        "    headers = [header.text for header in soup.find_all('th')]\n",
        "\n",
        "    # Extracting table rows\n",
        "    rows_data = []\n",
        "    for row in soup.find_all('tr'):\n",
        "        cells = row.find_all('td')\n",
        "        row_data = [cell.text.strip() for cell in cells]\n",
        "        if row_data:\n",
        "            rows_data.append(row_data)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(rows_data, columns=headers)\n",
        "\n",
        "    # Drop the \"Volume\" column if it exists\n",
        "    if \"Volume\" in df.columns:\n",
        "        df.drop([\"Volume\"], axis=1, inplace=True)\n",
        "\n",
        "    # Convert the 'Open' column to string type to handle None/NaN values\n",
        "    df['Open'] = df['Open'].astype(str)\n",
        "\n",
        "    # Remove rows with stock splits by filtering out rows where 'Open' column contains 'Split'\n",
        "    df = df[~df['Open'].str.contains(\"Split\")]\n",
        "\n",
        "    # Set the \"Date\" column as the index if it exists\n",
        "    if \"Date\" in df.columns:\n",
        "        df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "    # Define the columns to keep, just the closing price\n",
        "    columns_to_keep = ['Close*']\n",
        "\n",
        "    # Filter out columns to only keep the ones specified\n",
        "    df = df[columns_to_keep]\n",
        "\n",
        "    # Rename columns as needed\n",
        "    df.rename(columns={'Close*' : 'Closing Price'}, inplace=True)\n",
        "    df = df[:-1]\n",
        "    # Save the DataFrame to the given CSV file path\n",
        "    df.to_csv(\"updatedTSLA.csv\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Call the function to extract data and save it to a CSV file\n",
        "extra_source1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB3qXt_XuY7b"
      },
      "outputs": [],
      "source": [
        "# Define further extra source functions as necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uttEYrm9US5s"
      },
      "source": [
        "#Inconsistencies\n",
        "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
        "\n",
        "1. In the downloaded dataset that we selected, there were many inconsistencies.\n",
        "    * NaN values / Unnecessary values\n",
        "        * The missing data values created issues when attempting to sort the data frame and also made the data set more complicated.\n",
        "        * Moreover, there were unnecessary columns that we simply did not need. These held information such as engine size and sustainability scores.\n",
        "        * These were omitted using slicing methods where columns to keep were redefined.\n",
        "    * Duplicate Values\n",
        "        * Many rows had duplicate values (same cars with the same year)\n",
        "        * Inorder to identify and resolve this issue, we performed a group by and aggregate function to determine which duplicate was kept and which was removed.\n",
        "        * Many of the cars had the same make and model, fuel efficiency, etc..., but just a serial number. There were also grouped together and all the numerical values were averaged.\n",
        "    * Inconsistent rounding\n",
        "        * Throughout the data set, many of the numerical values were unrounded and had dozens of decimal values.\n",
        "        * This caused issues with reading the dataset because it pushed all the following value.\n",
        "        * To clean this, we used a .round() function to round each decimal point to either 2 or 4 decimal values.\n",
        "    * Cleaning date\n",
        "        * The data set started in the 20th century and to best align with our other data sets, we trimmed the date to start at the 2000's.\n",
        "        * This was done with a simple if statement and slicing.\n",
        "        * Significantly improved the legibility of the data frame. \n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n",
        "4. In the extra Yahoo Finance html data set, these were the inconsistencies:\n",
        "    * Error Values\n",
        "        * The volume column was full of errors and \"#####\" in place of numerical values.\n",
        "        * We did not need this column so it was removed from the data set.\n",
        "        * This was done using Beautiful Soup, Pandas, and simply dropping the column using its name.\n",
        "    * Missing Values\n",
        "        * Some rows had values that were empty.\n",
        "        * To spot this, we looked for data values with NaN or null.\n",
        "        * These data sets were removed.\n",
        "    * Inconsistent Rows formatting\n",
        "        * Some of the rows had the string \"Stock Split\" written in the column where there should be numerical values.\n",
        "        * This would make it very hard to compute numerical calculations to columns.\n",
        "        * These rows were removed from the dataset using Pandas drop function.\n",
        "\n",
        "5. (if applicable)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GJ4Xt1uf1LT7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
